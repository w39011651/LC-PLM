{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea6643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_preprocess\n",
    "\n",
    "data = data_preprocess.run()\n",
    "\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "for item in data:\n",
    "    while len(item['sequence']) > MAX_LEN:\n",
    "        sequences.append(item['sequence'][:MAX_LEN])\n",
    "        labels.append(item['label'][:MAX_LEN])\n",
    "        item['sequence'] = item['sequence'][MAX_LEN:]\n",
    "        item['label'] = item['label'][MAX_LEN:]\n",
    "    sequences.append(item['sequence'])\n",
    "    labels.append(item['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3e78f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample amount\n",
       "0          35045\n",
       "1           1131"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "zero_counter = 0\n",
    "ones_counter = 0\n",
    "\n",
    "for l in labels:\n",
    "    zero_counter+=l.count(0)\n",
    "    ones_counter+=l.count(1)\n",
    "\n",
    "pd.DataFrame(columns=['sample amount'], data = [zero_counter, ones_counter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f36fdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lovem/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset import FADBindingDataset, split_dataset\n",
    "from esm_model import EsmForSequenceLabeling\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "model = EsmForSequenceLabeling(\"facebook/esm2_t6_8M_UR50D\", 2)\n",
    "dataset = FADBindingDataset(tokenizer, sequences, labels, max_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc76d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 0, 20, 15, 12, 10, 18,  7, 18, 12,  4,  8,  7,  4, 12,  8,  6,  7, 23,\n",
       "         23, 12,  8, 15, 17,  7,  8, 10, 10,  7,  5, 17, 10, 20, 11,  5, 21,  8,\n",
       "         10, 18,  4, 18,  7, 21, 13, 15, 19, 15, 10, 17, 15, 17, 18, 15,  4, 15,\n",
       "         17, 17, 15,  9,  9, 17, 17, 18, 12, 17,  4, 19, 11,  7, 15, 17, 14,  4,\n",
       "         15, 23, 15, 12,  7, 13, 15, 12, 17,  4,  7, 10, 14, 17,  8, 14, 17,  9,\n",
       "          7, 19, 21,  4,  9, 12, 17, 21, 17,  6,  4, 18, 15, 19,  4,  9,  6, 21,\n",
       "         11, 23,  6, 12, 12, 14, 19, 19, 17,  9,  4, 13, 17, 17, 14, 17, 17, 16,\n",
       "         12,  2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1836b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "[train_dataset, test_dataset] = split_dataset(dataset, 0.8)\n",
    "[validate_dataset, test_dataset] = split_dataset(test_dataset, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8aec485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "\n",
    "def find_best_evalution(logits, labels):\n",
    "    logits_np = np.array(logits)\n",
    "    labels_np = np.array(labels)\n",
    "    #sigmoid probability\n",
    "    print(logits_np.shape)\n",
    "    probs = 1 / (1 + np.exp(-logits_np[:, :, 1]))#[batch size, sequence length]\n",
    "\n",
    "    labels_flat = labels_np.flatten()\n",
    "    probs_flat = probs.flatten()\n",
    "\n",
    "     # 只取非 padding (-100) 的有效樣本\n",
    "    active_indices = labels_flat != -100\n",
    "    final_labels = labels_flat[active_indices]\n",
    "    final_probs = probs_flat[active_indices]\n",
    "\n",
    "    # 計算 ROC 並找出最佳 G-mean 閾值\n",
    "    fpr, tpr, thresholds = roc_curve(final_labels, final_probs)\n",
    "    gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "    ix = np.argmax(gmeans)\n",
    "    best_threshold = thresholds[ix]\n",
    "\n",
    "    print(f'Best Threshold: {best_threshold:.3f}, G-Mean: {gmeans[ix]:.3f}')\n",
    "    return best_threshold\n",
    "    \n",
    "\n",
    "def custom_classification_report(logits, labels, test_evalution = False):\n",
    "    \"\"\"Filt the ignore value and return classification report\"\"\"\n",
    "    labels_np = np.array(labels)\n",
    "    logits_np = np.array(logits)\n",
    "    #preds = np.argmax(logits_np, axis=-1).flatten()\n",
    "    if test_evalution:\n",
    "        threshold = find_best_evalution(logits, labels)\n",
    "    else:\n",
    "        threshold = 0.5\n",
    "    probs = 1/(1+np.exp(-logits_np[:,:,1]))\n",
    "    preds = (probs > threshold).astype(int).flatten()\n",
    "    labels_flat = labels_np.flatten()\n",
    "\n",
    "    print(\"Shape of flattened labels:\", labels_flat.shape)\n",
    "    print(\"Shape of flattened predictions:\", preds.shape)\n",
    "\n",
    "    active_indices = labels_flat != -100\n",
    "    final_labels = labels_flat[active_indices]\n",
    "    final_preds = preds[active_indices]\n",
    "    report_str = classification_report(final_labels, final_preds)\n",
    "    report_dict = classification_report(final_labels, final_preds, output_dict=True)\n",
    "    print(report_str)\n",
    "    return report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d051a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "def train_by_dataloader():\n",
    "\n",
    "    writer = SummaryWriter('runs/my_experiment') # TensorBoard writer\n",
    "\n",
    "    ## hyperparameters in yaml file\n",
    "    with open(\"../config/mamba_config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    learning_rate = config[\"training_arguments\"][\"learning_rate\"]\n",
    "    per_device_train_batch_size = config[\"training_arguments\"][\"per_device_train_batch_size\"]\n",
    "    num_epochs = config[\"training_arguments\"][\"num_train_epochs\"]\n",
    "    warmup_steps = config['training_arguments']['warmup_steps']\n",
    "    log_step = config['training_arguments']['eval_steps']\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    ##\n",
    "\n",
    "    ## create dataloader\n",
    "    train_loader = DataLoader(train_dataset, per_device_train_batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(validate_dataset, per_device_train_batch_size, shuffle=True)\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    ## \n",
    "\n",
    "    ## some variable to use\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    best_f1_score = -float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state_dict = None\n",
    "    global_step = 0\n",
    "    ## \n",
    "\n",
    "    ## move model to GPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    ##\n",
    "  \n",
    "    for epoch in range(num_epochs):\n",
    "        \"\"\"Training Loop Start here\"\"\"\n",
    "        model.train()  \n",
    "        total_train_loss = 0  \n",
    "            \n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):          \n",
    "            inputs = {k:v.to(device) for k,v in batch.items() if k != 'labels'}# 將批次數據移至設備  \n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()# 清除梯度\n",
    "            loss, _ = model(**inputs, labels=labels)  # 前向傳播  \n",
    "            loss.backward()  # 反向傳播和優化  \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            global_step+=1\n",
    "\n",
    "            if global_step % log_step == 0:\n",
    "                avg_train_loss = total_train_loss / (step + 1)\n",
    "                writer.add_scalar('Loss/train', avg_train_loss, global_step)\n",
    "                print(f\"Epoch {epoch+1}, Step {step+1}, Training Loss: {avg_train_loss:.6f}\")\n",
    "                \n",
    "            total_train_loss += loss.item()\n",
    "        \"\"\"Training Loop End here\"\"\"\n",
    "\n",
    "        \"\"\"Validate Loop Start here\"\"\"\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        all_val_logits = []\n",
    "        all_val_labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(validation_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                loss, logits = model(**inputs, labels=labels)\n",
    "\n",
    "                total_eval_loss += loss.item()\n",
    "                all_val_logits.extend(logits.cpu().numpy())\n",
    "                all_val_labels.extend(labels.cpu().numpy())\n",
    "        \"\"\"Validate Loop End here\"\"\"\n",
    "\n",
    "        \"\"\"Report and Save the weight of the model\"\"\"\n",
    "        report = custom_classification_report(all_val_logits, all_val_labels, True)\n",
    "\n",
    "        avg_eval_loss = total_eval_loss / len(validation_loader)\n",
    "        val_f1_score = report['1']['f1-score']##means Marco-f1-score on 1's sample prediction\n",
    "        print(f\"Validation Loss: {avg_eval_loss:.6f}\")\n",
    "\n",
    "        # Store the model has best performance on the specific indicator like: \n",
    "        # evaluation loss, recall rate or f1-score\n",
    "        ## early stopping part\n",
    "        if avg_eval_loss < best_val_loss:\n",
    "            best_val_loss = avg_eval_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs!\")\n",
    "                break\n",
    "\n",
    "        ## save weight part\n",
    "        if val_f1_score > best_f1_score:\n",
    "            best_f1_score = val_f1_score\n",
    "            best_model_state_dict = model.state_dict() # 保存最佳權重\n",
    "        \"\"\"An epoch end here\"\"\"\n",
    "\n",
    "        writer.add_scalar('Loss/val', avg_eval_loss, epoch + 1)\n",
    "    writer.close()\n",
    "\n",
    "    if best_model_state_dict is not None:\n",
    "        model.load_state_dict(best_model_state_dict)\n",
    "        print(\"Loaded best model weights!\")\n",
    "    else:\n",
    "        print(\"No improvement found during training.\")\n",
    "    # 保存微調後的模型  \n",
    "    torch.save(model.state_dict(), \"esm_fad_binding_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6333fe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0031516551971436\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "time.sleep(3)\n",
    "et = time.time()\n",
    "print(et - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ea48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/63 [00:00<?, ?it/s]/home/lovem/miniconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "Epoch 1:  63%|██████▎   | 40/63 [00:07<00:03,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 40, Training Loss: 0.163316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 63/63 [00:10<00:00,  5.89it/s]\n",
      "Validation Epoch 1: 100%|██████████| 8/8 [00:00<00:00, 51.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 2)\n",
      "Best Threshold: 0.441, G-Mean: 0.541\n",
      "Shape of flattened labels: (4096,)\n",
      "Shape of flattened predictions: (4096,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.66      0.78      3660\n",
      "           1       0.07      0.44      0.12       205\n",
      "\n",
      "    accuracy                           0.65      3865\n",
      "   macro avg       0.51      0.55      0.45      3865\n",
      "weighted avg       0.91      0.65      0.74      3865\n",
      "\n",
      "Validation Loss: 0.142362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/63 [00:00<?, ?it/s]/home/lovem/miniconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "Epoch 2:  27%|██▋       | 17/63 [00:02<00:06,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 17, Training Loss: 0.125025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  90%|█████████ | 57/63 [00:08<00:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 57, Training Loss: 0.107299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 63/63 [00:09<00:00,  6.86it/s]\n",
      "Validation Epoch 2: 100%|██████████| 8/8 [00:00<00:00, 53.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 2)\n",
      "Best Threshold: 0.387, G-Mean: 0.578\n",
      "Shape of flattened labels: (4096,)\n",
      "Shape of flattened predictions: (4096,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.64      0.77      3660\n",
      "           1       0.07      0.52      0.13       205\n",
      "\n",
      "    accuracy                           0.63      3865\n",
      "   macro avg       0.52      0.58      0.45      3865\n",
      "weighted avg       0.91      0.63      0.73      3865\n",
      "\n",
      "Validation Loss: 0.070801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/63 [00:00<?, ?it/s]/home/lovem/miniconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "Epoch 3:  54%|█████▍    | 34/63 [00:04<00:04,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 34, Training Loss: 0.043888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 63/63 [00:09<00:00,  6.93it/s]\n",
      "Validation Epoch 3: 100%|██████████| 8/8 [00:00<00:00, 50.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 2)\n",
      "Best Threshold: 0.230, G-Mean: 0.481\n",
      "Shape of flattened labels: (4096,)\n",
      "Shape of flattened predictions: (4096,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.39      0.55      3660\n",
      "           1       0.05      0.59      0.09       205\n",
      "\n",
      "    accuracy                           0.40      3865\n",
      "   macro avg       0.50      0.49      0.32      3865\n",
      "weighted avg       0.90      0.40      0.53      3865\n",
      "\n",
      "Validation Loss: 0.017190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/63 [00:00<?, ?it/s]/home/lovem/miniconda3/lib/python3.9/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "Epoch 4:  17%|█▋        | 11/63 [00:01<00:07,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 11, Training Loss: 0.009927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  33%|███▎      | 21/63 [00:03<00:06,  6.80it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_by_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 45\u001b[0m, in \u001b[0;36mtrain_by_dataloader\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \n\u001b[1;32m     43\u001b[0m total_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)):          \n\u001b[1;32m     46\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {k:v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m}\u001b[38;5;66;03m# 將批次數據移至設備  \u001b[39;00m\n\u001b[1;32m     47\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:154\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m--> 154\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate({key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:154\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m--> 154\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate({key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:213\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    211\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    212\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils import helpers\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "helpers.log(\"Start Training\")\n",
    "trainer = train_by_dataloader()\n",
    "helpers.log(\"End Training\")\n",
    "end_time = time.time()\n",
    "helpers.log(f\"Total Training Time:{end_time - start_time} second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ac362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 30.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 2)\n",
      "Best Threshold: 0.143, G-Mean: 0.763\n",
      "Shape of flattened labels: (4096,)\n",
      "Shape of flattened predictions: (4096,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.83      0.90      3635\n",
      "           1       0.12      0.69      0.21       124\n",
      "\n",
      "    accuracy                           0.82      3759\n",
      "   macro avg       0.55      0.76      0.55      3759\n",
      "weighted avg       0.96      0.82      0.88      3759\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 128.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128, 2)\n",
      "Best Threshold: 0.214, G-Mean: 0.788\n",
      "Shape of flattened labels: (4096,)\n",
      "Shape of flattened predictions: (4096,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.91      0.95      3713\n",
      "           1       0.10      0.67      0.17        57\n",
      "\n",
      "    accuracy                           0.90      3770\n",
      "   macro avg       0.55      0.79      0.56      3770\n",
      "weighted avg       0.98      0.90      0.94      3770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_loader = DataLoader(validate_dataset, batch_size = 4, shuffle = False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 4, shuffle=False)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.eval()\n",
    "total_eval_loss = 0\n",
    "all_val_logits = []\n",
    "all_val_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(validation_loader):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        _, logits = model(**inputs)\n",
    "        all_val_logits.extend(logits.cpu().numpy())\n",
    "        all_val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "custom_classification_report(all_val_logits, all_val_labels, True)\n",
    "all_val_logits = []\n",
    "all_val_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        _, logits = model(**inputs)\n",
    "        all_val_logits.extend(logits.cpu().numpy())\n",
    "        all_val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "custom_classification_report(all_val_logits, all_val_labels, True)\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
